{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ['llama', 'gpt', 'deepseek'][1]\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from glob import glob\n",
    "import json\n",
    "from num2words import num2words\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = ChatOllama(\n",
    "        model=\"llama3.2\",\n",
    "        temperature=0,\n",
    "        num_ctx=4096,\n",
    "        num_predict=2048,\n",
    "        verbose=True\n",
    "    )\n",
    "model = None\n",
    "if model_name == 'llama':\n",
    "    model = llama\n",
    "elif model_name == 'deepseek':\n",
    "    model = ChatOllama(\n",
    "        model=\"deepseek-r1\",\n",
    "        temperature=0,\n",
    "        num_ctx=4096,\n",
    "        num_predict=2048,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    model = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "        # base_url=\"...\",\n",
    "        # organization=\"...\",\n",
    "        # other params...\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('./selected_questions/*.jsonl')\n",
    "questions = []\n",
    "for path in files:\n",
    "    question_type = path[path.rfind('/')+1:-6]\n",
    "    with open(path, 'r') as file:\n",
    "        for i in range(100):\n",
    "            line = file.readline()\n",
    "            question = json.loads(line)\n",
    "            question['type'] = question_type\n",
    "            questions.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt0 = str(\"Answer the provided user question. The answer should not exceed twenty words.\")\n",
    "\n",
    "\n",
    "system_prompt1 = str(\"Answer the provided user question while satisfying the following requirements:\\n\"\n",
    "                    \"1. do not include any parts of the question in the answer you must provide the answer directly.\\n\"\n",
    "                    \"2. provide only the property the user is asking for like name of an entity, its location, distance, direction.\\n\"\n",
    "                    \"3. don't provide information the user didn't ask for.\\n\"\n",
    "                    \"4. any number must be written as words and rounded to the nearest ten.\\n\"\n",
    "                    \"5. only use metric units.\")\n",
    "\n",
    "\n",
    "system_prompt2 = str(\"Given a question and a text answer, parse the text answer to json format.\"\n",
    "                     \" The location must be provided as a complete address,\"\n",
    "                     \" any measurment must be in metric units,\"\n",
    "                     \" and directions must be converted to azimuth angle in degress.\"\n",
    "                     \" Try to match the following schema:\"\n",
    "                     \"\"\"\n",
    "                        {\n",
    "                            \"name\" string\n",
    "                            \"address\": string,\n",
    "                            \"count\": integer,\n",
    "                            \"distance\": integer,\n",
    "                            \"length\": integer,\n",
    "                            \"area\": integer,\n",
    "                            \"azimuth_angle\": integer,\n",
    "                            %OTHER_ATT%\n",
    "                        }\n",
    "                    If a value is missing don't include it in the output, and don't write any comments.\n",
    "                    All json blocks must be enclosed with ```json and ```\n",
    "                     \"\"\")\n",
    "\n",
    "new_prompt = \"\"\"Given a question and a text answer, parse the text answer to json format. The schema must match the provided schema. If an attribute doesn't have a related value in the answer, you can skip it.\"\"\"\n",
    "json_schema = {\n",
    "  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "  \"title\": \"answer\",\n",
    "  \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"name\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The name of the entity, like name of the restuarant, park, etc.\"\n",
    "      },\n",
    "      \"address\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The full address of the entity.\"\n",
    "      },\n",
    "      \"distance\": {\n",
    "          \"type\": \"number\",\n",
    "          \"description\": \"The distance in meters from the location mentioned in the question to this entity.\"\n",
    "      },\n",
    "      \"length\": {\n",
    "          \"type\": \"number\",\n",
    "          \"description\": \"The length in meters of the entity that matches the descirption in the question.\"\n",
    "      },\n",
    "      \"area\": {\n",
    "          \"type\": \"number\",\n",
    "          \"description\": \"The area of the entity in meters squared that matches the descirption in the question.\"\n",
    "      },\n",
    "      \"azimuth_angle\": {\n",
    "          \"type\": \"number\",\n",
    "          \"description\": \"The azimuth angle at which the entity can be found from the location mentioned in the question.\"\n",
    "      },\n",
    "      \"Nickname\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The nickname of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Architect\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The name of the architect of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Established\": {\n",
    "        \"type\": \"number\",\n",
    "        \"description\": \"The year the entity that matches the question was established.\"\n",
    "      },\n",
    "      \"Director\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The of the name of the director of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Date opened\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The date at which the entity that matches the question was opened.\"\n",
    "      },\n",
    "      \"Nearest city\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The nearest city to the entity that matches the question.\"\n",
    "      },\n",
    "      \"Opened\": {\n",
    "        \"type\": \"number\",\n",
    "        \"description\": \"The year at which the entity that matches the question was opened.\"\n",
    "      },\n",
    "      \"Motto\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The motto of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Designed by\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The name of the designer of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Affiliated university\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The name of a university affiliated with the entity that matches the question.\"\n",
    "      },\n",
    "      \"Former names\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"A former name of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Helipad\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Description about weather the the entity that matches the question has a helipad.\"\n",
    "      },\n",
    "      \"Capacity\": {\n",
    "        \"type\": \"number\",\n",
    "        \"description\": \"The capacity of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Opening date\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The opening date of the entity that matches the question.\"\n",
    "      },\n",
    "      \"Built\": {\n",
    "        \"type\": \"number\",\n",
    "        \"description\": \"The year the entity that matches the question was built.\"\n",
    "      },\n",
    "      \"Mascot\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The name of the mascot of the entity that matches the question.\"\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    name: Optional[str]\n",
    "    address: Optional[str]\n",
    "    distance: Optional[float] = None\n",
    "    length: Optional[float] = None\n",
    "    area: Optional[float] = None\n",
    "    count: Optional[int] = None\n",
    "    azimuth_angle: Optional[float] = None\n",
    "    nickname: Optional[str] = None\n",
    "    architect: Optional[str] = None\n",
    "    established: Optional[int] = None\n",
    "    director: Optional[str] = None\n",
    "    date_opened: Optional[str] = None\n",
    "    nearest_city: Optional[str] = None\n",
    "    opened: Optional[int] = None\n",
    "    motto: Optional[str] = None\n",
    "    designed_by: Optional[str] = None\n",
    "    affiliated_university: Optional[str] = None\n",
    "    former_names: Optional[str] = None\n",
    "    helipad: Optional[str] = None\n",
    "    capacity: Optional[float] = None\n",
    "    opening_date: Optional[str] = None\n",
    "    built: Optional[int] = None\n",
    "    mascot: Optional[str] = None\n",
    "\n",
    "\n",
    "class Answers(BaseModel):\n",
    "    answers: List[Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            SystemMessage(content=system_prompt1),\n",
    "            HumanMessage(content=questions[1454]['question'])\n",
    "        ]\n",
    "a = model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./answers_%s.json' % model_name, 'r') as file:\n",
    "    _answers = json.loads(file.read())\n",
    "    _answers = {a['id']: a for a in _answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2800/2800 [00:00<00:00, 2758114.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "answers = []\n",
    "for q in tqdm(questions):\n",
    "    if q['id'] in _answers:\n",
    "        answers.append(_answers[q['id']])\n",
    "    else:\n",
    "        messages = [\n",
    "                SystemMessage(content=system_prompt1),\n",
    "                HumanMessage(content=q['question'])\n",
    "            ]\n",
    "        answers.append({'content': model.invoke(messages).content, 'id': q['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./answers_%s.json' % model_name, 'w') as file:\n",
    "#     file.write(json.dumps(answers, indent=2))\n",
    "with open('./answers_%s.json' % model_name, 'r') as file:\n",
    "    answers = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json_answers_%s.json' % model_name, 'r') as file:\n",
    "        _json_answers = json.loads(file.read())\n",
    "        _json_answers = {a['id']: a for a in _json_answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2800/2800 [00:00<00:00, 3029940.97it/s]\n"
     ]
    }
   ],
   "source": [
    "json_answers = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    q = questions[i]\n",
    "    if q['id'] in _json_answers:\n",
    "        json_answers.append(_json_answers[q['id']])\n",
    "    else:\n",
    "        if 'multihop1' in q['type']:\n",
    "            sys_prompt= system_prompt2.replace('%OTHER_ATT%', '\"%s\": string' % q['answers'][0]['multihop_attribute'])\n",
    "        else:\n",
    "            sys_prompt= system_prompt2.replace('%OTHER_ATT%', '')\n",
    "        a = answers[i]\n",
    "        if a['id'] != q['id']:\n",
    "            for j in range(len(answers)):\n",
    "                if answers[j]['id'] == q['id']:\n",
    "                    a = answers[j]\n",
    "                    break\n",
    "        messages = [\n",
    "                SystemMessage(content=sys_prompt),\n",
    "                HumanMessage(content=\"Question: %s\\nAnswer: %s\" % (q['question'], a['content']))\n",
    "            ]\n",
    "        json_answers.append({'content': llama.invoke(messages).content, 'id': q['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./json_answers_%s.json' % model_name, 'w') as file:\n",
    "#         file.write(json.dumps(json_answers, indent=2))\n",
    "with open('./json_answers_%s.json' % model_name, 'r') as file:\n",
    "        json_answers = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def flatten_if_nested(array):\n",
    "    # Check if the input is a list and contains nested lists\n",
    "    if isinstance(array, list) and any(isinstance(item, list) for item in array):\n",
    "        flattened = []\n",
    "        for item in array:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(flatten_if_nested(item))\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "    else:\n",
    "        return array  # Return the input as-is if it's not a list or doesn't contain nested lists\n",
    "\n",
    "def extract_json_blocks(text, i):\n",
    "    # Regular expression pattern to match JSON blocks\n",
    "    pattern = r'```[\\s]*json(.*?)```'\n",
    "    pattern1 = r'\\b\\d+(?:_\\d+)*\\b'\n",
    "    pattern2 = r'\\b\\d+(?:,\\d+)*\\b'\n",
    "    pattern3 = r'//.*?\\n'\n",
    "    pattern4 = r',\\s*}'\n",
    "    pattern5 = r'}\\s*{'\n",
    "    # Find all JSON blocks\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Parse each match to ensure valid JSON\n",
    "    json_blocks = []\n",
    "    for match in matches:\n",
    "        try:\n",
    "            # Remove any leading/trailing whitespace and parse as JSON\n",
    "            s = match.strip()\n",
    "            s = re.sub(pattern1, lambda x: x.group().replace('_', ''), s)\n",
    "            s = re.sub(pattern2, lambda x: x.group().replace(',', ''), s)\n",
    "            s = re.sub(pattern3, '', s)\n",
    "            s = re.sub(pattern4, '}', s)\n",
    "            s = s.replace('''\\\\\\'''', '''\\'''').replace('''\\\\&''', '''&''').replace('}\\njson', '}')\n",
    "            if re.search(pattern5, s):\n",
    "                s = re.sub(pattern5, '},\\n{', s)\n",
    "                s = '[\\n%s\\n]' % s\n",
    "            convert_area = False\n",
    "            if 'acres' in s:\n",
    "                convert_area = True\n",
    "                s = s.replace(' acres,', ',')\n",
    "            json_data = json.loads(s)\n",
    "            if convert_area and 'area' in json_data:\n",
    "                json_data['area'] = json_data['area'] * 4046.8564224\n",
    "            json_blocks.append(json_data)\n",
    "        except json.JSONDecodeError as w:\n",
    "            print(w)\n",
    "            # If parsing fails, print an error message (can log or handle as needed)\n",
    "            print(i)\n",
    "            print(s)\n",
    "            print(\"Warning: Found an invalid JSON block.\") \n",
    "    return flatten_if_nested(json_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_answers = []\n",
    "for i in range(len(questions)):\n",
    "    q = questions[i]\n",
    "    a = json_answers[i]\n",
    "    if a['id'] != q['id']:\n",
    "        for j in range(len(json_answers)):\n",
    "            if json_answers[j]['id'] == q['id']:\n",
    "                a = json_answers[j]\n",
    "                break\n",
    "    parsed_answers.append(extract_json_blocks(a['content'], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from pyproj import Geod\n",
    "\n",
    "geod = Geod(ellps='WGS84')\n",
    "geocoder = Nominatim(user_agent=\"Geocoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/majid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/majid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/majid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluate\n",
    "importlib.reload(evaluate)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_evaluation = []\n",
    "\n",
    "# evaluate text answers\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    q = questions[i]\n",
    "    a = answers[i]\n",
    "    if a['id'] != q['id']:\n",
    "        for j in range(len(answers)):\n",
    "            if answers[j]['id'] == q['id']:\n",
    "                a = answers[j]\n",
    "                break\n",
    "    text_answer = a['content']\n",
    "    key = ''\n",
    "    if 'multihop1' in q['type']:\n",
    "        key = 'multihop_long_answer'\n",
    "    elif 'name' in q['type']:        \n",
    "        key = 'name'\n",
    "    elif 'loc' in q['type']:\n",
    "        key = 'address'\n",
    "    elif 'angle' in q['type']:\n",
    "        key = 'angle_description'\n",
    "    elif 'area' in q['type']:\n",
    "        key = 'area'\n",
    "    elif 'length' in q['type']:\n",
    "        key = 'length'\n",
    "    elif 'count' in q['type']:\n",
    "        key = 'count'\n",
    "    elif 'distance' in q['type']:\n",
    "        key = 'distance'\n",
    "    true_answer = []\n",
    "    for a in q['answers']:\n",
    "        v = evaluate.get_osm_value(a, key)\n",
    "        if v == None:\n",
    "            continue\n",
    "        if key in ['area', 'length', 'count', 'distance']:\n",
    "            v = num2words(v)\n",
    "        if 'area' == key:\n",
    "            v += ' meters squared'\n",
    "        elif key in ['length', 'distance']:\n",
    "            v += ' meters'\n",
    "        true_answer.append(v)\n",
    "    if len(text_answer):\n",
    "        true_answer = '\\n'.join(true_answer)\n",
    "        P, R, F1 = evaluate.evaluate_entity_names(text_answer, true_answer)\n",
    "        text_evaluation.append({'attempted': True, 'P': P, 'R': R, 'F1': F1})\n",
    "    else:\n",
    "        text_evaluation.append({'attempted': False, 'P': 0, 'R': 0, 'F1': 0})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(text_evaluation)\n",
    "df['type'] = [q['type'] for q in questions]\n",
    "df['id'] = [q['id'] for q in questions]\n",
    "df.to_csv(f'./{model_name}_text_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the heading to the closest university from Santa Barbara Museum of Natural History Sea Center, Santa Barbara, CA?\n",
      "{'content': 'Northwest.', 'id': 1438}\n",
      "[{'name': 'Santa Barbara Museum of Natural History Sea Center', 'address': '100 N Harbor Way, Santa Barbara, CA 93109', 'distance': None, 'length': None, 'area': None, 'azimuth_angle': 315}]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(questions)):\n",
    "    q = questions[i]\n",
    "    if q['id'] != 1438:\n",
    "        continue\n",
    "    a = answers[i]\n",
    "    p = parsed_answers[i]\n",
    "    if a['id'] != q['id']:\n",
    "        for j in range(len(answers)):\n",
    "            if answers[j]['id'] == q['id']:\n",
    "                a = answers[j]\n",
    "                p = parsed_answers[j]\n",
    "                break\n",
    "    if q['id'] == 1438:\n",
    "        break\n",
    "print(q['question'])\n",
    "print(a)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('northwest', 'Northwest.', ['Northwest'], ['northwest'], (0, 0, 0))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import normalize_text\n",
    "v = evaluate.get_osm_value(q['answers'][0], 'angle_description')\n",
    "\n",
    "v, a['content'], normalize_text(a['content']), normalize_text(v), evaluate.evaluate_entity_names(a['content'], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '```json\\n{\\n  \"name\": \"University of California, Santa Barbara\",\\n  \"address\": \"Santa Barbara, CA\",\\n  \"distance\": null,\\n  \"length\": null,\\n  \"area\": null,\\n  \"azimuth_angle\": null\\n}\\n```\\n\\nNote: The answer does not provide enough information to determine the distance, length, area, or azimuth angle.',\n",
       " 'id': 1438}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_answers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2800/2800 [00:02<00:00, 1075.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluate parsed_answers\n",
    "parsed_evaluation = []\n",
    "progress = tqdm.tqdm(range(len(questions)))\n",
    "def imporved_f1(new_f1, scores):\n",
    "    return ('F1' not in scores) or (new_f1 > scores['F1'])\n",
    "\n",
    "for i in progress:\n",
    "    q = questions[i]\n",
    "    parsed_answer = parsed_answers[i]\n",
    "    scores = {'attempted': False}\n",
    "    if 'multihop1' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'multihop_answer')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_answer = p.get(a['multihop_attribute'], None)\n",
    "                if pred_answer == None or len(pred_answer) == 0:\n",
    "                    continue\n",
    "                P, R, F1 = evaluate.evaluate_entity_names(pred_answer, v)\n",
    "                if imporved_f1(F1, scores):\n",
    "                    scores = {'attempted': True, 'P': P, 'R': R, 'F1': F1}\n",
    "    elif 'name' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'name')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_answer = p.get('name', None)\n",
    "                if pred_answer == None or len(pred_answer) == 0:\n",
    "                    continue\n",
    "                P, R, F1 = evaluate.evaluate_entity_names(pred_answer, v)\n",
    "                if imporved_f1(F1, scores):\n",
    "                    scores = { 'attempted': True, 'P': P, 'R': R, 'F1': F1}\n",
    "    elif 'loc' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'address')\n",
    "            loc = evaluate.get_osm_value(a, 'location')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_answer = p.get('address', None)\n",
    "                if pred_answer == None or len(pred_answer) == 0:\n",
    "                    continue\n",
    "                if type(pred_answer) == type([]):\n",
    "                    pred_answer = ', '.join(pred_answer)\n",
    "                P, R, F1 = evaluate.evaluate_entity_names(pred_answer, v)\n",
    "                if imporved_f1(F1, scores):\n",
    "                    scores.update({'attempted': True,'P': P, 'R': R, 'F1': F1})\n",
    "                pred_loc = evaluate.get_location_by_address(geocoder, pred_answer)\n",
    "                if pred_loc == None:\n",
    "                    continue\n",
    "                distance_error = evaluate.evaluate_location(geod, [pred_loc], [loc])[0]\n",
    "                distance_limit = 5*10**5\n",
    "                if distance_error > distance_limit:\n",
    "                    distance_error = 1.0\n",
    "                else:\n",
    "                    distance_error /= distance_limit\n",
    "                if distance_error < scores.get('distance_error', float('inf')):\n",
    "                    scores['distance_error'] = distance_error\n",
    "    elif 'angle' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            angle = evaluate.get_osm_value(a, 'angle')\n",
    "            angle_desc = evaluate.get_osm_value(a, 'angle_description')\n",
    "            if angle == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_angle = p.get('azimuth_angle', None)\n",
    "                try:\n",
    "                    pred_angle = int(pred_angle)\n",
    "                except:\n",
    "                    continue\n",
    "                pred_answer = evaluate.get_angle_desc(pred_angle)\n",
    "                if pred_answer == None or len(pred_answer) == 0:\n",
    "                    continue\n",
    "                P, R, F1 = evaluate.evaluate_entity_names(pred_answer, angle_desc)\n",
    "                if imporved_f1(F1, scores):\n",
    "                    scores.update({'attempted': True,'P': P, 'R': R, 'F1': F1})\n",
    "                angle_error = evaluate.evaluate_angle([pred_angle], [angle])[0]\n",
    "                if angle_error < scores.get('angle_error', float('inf')):\n",
    "                    scores['angle_error'] = angle_error\n",
    "    elif 'area' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'area')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_v = p.get('area', None)\n",
    "                if pred_v == None:\n",
    "                    continue\n",
    "                try:\n",
    "                    pred_v = int(pred_v)\n",
    "                except:\n",
    "                    continue\n",
    "                relative_error = evaluate.evaluate_measurement(pred_v, v)\n",
    "                if relative_error < scores.get('relative_error', float('inf')):\n",
    "                    scores['relative_error'] = relative_error\n",
    "                    scores['attempted'] = True\n",
    "    elif 'length' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'length')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_v = p.get('length', None)\n",
    "                if pred_v == None:\n",
    "                    continue\n",
    "                try:\n",
    "                    pred_v = int(pred_v)\n",
    "                except:\n",
    "                    continue\n",
    "                relative_error = evaluate.evaluate_measurement(pred_v, v)\n",
    "                if relative_error < scores.get('relative_error', float('inf')):\n",
    "                    scores['relative_error'] = relative_error\n",
    "                    scores['attempted'] = True\n",
    "    elif 'count' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'count')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_v = p.get('count', None)\n",
    "                if pred_v == None:\n",
    "                    continue\n",
    "                try:\n",
    "                    pred_v = int(pred_v)\n",
    "                except:\n",
    "                    continue\n",
    "                relative_error = evaluate.evaluate_measurement(pred_v, v)\n",
    "                if relative_error < scores.get('relative_error', float('inf')):\n",
    "                    scores['relative_error'] = relative_error\n",
    "                    scores['attempted'] = True\n",
    "    elif 'distance' in q['type']:\n",
    "        for a in q['answers']:\n",
    "            v = evaluate.get_osm_value(a, 'distance')\n",
    "            if v == None:\n",
    "                continue\n",
    "            for p in parsed_answer:\n",
    "                pred_v = p.get('distance', None)\n",
    "                if pred_v == None:\n",
    "                    continue\n",
    "                try:\n",
    "                    pred_v = int(pred_v)\n",
    "                except:\n",
    "                    continue\n",
    "                relative_error = evaluate.evaluate_measurement(pred_v, v)\n",
    "                if relative_error < scores.get('relative_error', float('inf')):\n",
    "                    scores['relative_error'] = relative_error\n",
    "                    scores['attempted'] = True\n",
    "    parsed_evaluation.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(parsed_evaluation)\n",
    "df['type'] = [q['type'] for q in questions]\n",
    "df['id'] = [q['id'] for q in questions]\n",
    "df.loc[df['P'].isna(), 'P'] = 0\n",
    "df.loc[df['R'].isna(), 'R'] = 0\n",
    "df.loc[df['F1'].isna(), 'F1'] = 0\n",
    "df.loc[df['distance_error'].isna(), 'distance_error'] = 1.0\n",
    "df.loc[df['angle_error'].isna(), 'angle_error'] = 1.0\n",
    "df.loc[df['relative_error'].isna(), 'relative_error'] = 1.0\n",
    "\n",
    "df.to_csv(f'./{model_name}_parsed_eval.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
