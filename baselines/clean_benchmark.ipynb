{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_ordered = ['range+name', 'range:non_spat_filter+name', 'range:direction+name', 'range:towards+name', \n",
    " 'knn+name', 'knn:non_spat_filter+name', 'knn+name+multihop1', 'knn+name+multihop2',\n",
    " 'knn:direction+name', 'knn:towards+name', 'intersects:area_max+name', 'intersects:length_max+name',\n",
    " 'range+loc', 'range:non_spat_filter+loc','range:direction+loc',  'range:towards+loc',\n",
    "  'knn+loc', 'knn:non_spat_filter+loc', 'knn:direction+loc',  'knn:towards+loc', \n",
    "  'range+angle', 'knn+angle',\n",
    "  'range+count', 'intersects+count',\n",
    "'range+distance', 'knn+distance', \n",
    "'intersects:area_total+area', 'intersects:length_total+length'\n",
    " ]\n",
    "type_labels = {types_ordered[i-1]: 'T%d' % i for i in range(1,29)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {\n",
    "    'G': pd.read_csv('./gpt_text_eval.csv').fillna(0),\n",
    "    'L': pd.read_csv('./llama_text_eval.csv').fillna(0),\n",
    "    'GT': pd.read_csv('./gpt_text2sql_text_eval.csv').fillna(0),\n",
    "    'GR': pd.read_csv('./gpt_rag_text_eval.csv').fillna(0),\n",
    "    'LT': pd.read_csv('llama_text2sql_text_eval.csv').fillna(0),\n",
    "    'LR': pd.read_csv('llama_rag_text_eval.csv').fillna(0),\n",
    "    'R': pd.read_csv('shuffled_text_eval.csv').fillna(0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = {\n",
    "    'G': pd.read_csv('./gpt_parsed_eval.csv').fillna(0),\n",
    "    'L': pd.read_csv('./llama_parsed_eval.csv').fillna(0),\n",
    "    'GT': pd.read_csv('./gpt_text2sql_parsed_eval.csv').fillna(0),\n",
    "    'LT': pd.read_csv('./llama_text2sql_parsed_eval.csv').fillna(0),\n",
    "    'GR': pd.read_csv('./gpt_rag_parsed_eval.csv').fillna(0),\n",
    "    'LR': pd.read_csv('./llama_rag_parsed_eval.csv').fillna(0),\n",
    "    'R': pd.read_csv('./shuffled_parsed_eval.csv').fillna(0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {\n",
    "    'G': {\n",
    "        'text': './answers_gpt.json',\n",
    "        'json': './json_answers_gpt.json'\n",
    "    },\n",
    "    'GT': {\n",
    "        'text': './text2sql_answers_gpt.json',\n",
    "        'json': './text2sql_json_answers_gpt.json'\n",
    "    }, \n",
    "    'GR': {\n",
    "        'text': './rag_answers_gpt.json',\n",
    "        'json': './rag_json_answers_gpt.json'\n",
    "    },   \n",
    "    'L': {\n",
    "        'text': './answers_llama.json',\n",
    "        'json': './json_answers_llama.json'\n",
    "    },\n",
    "    'LT': {\n",
    "        'text': './text2sql_answers_llama.json',\n",
    "        'json': './text2sql_json_answers_llama.json'\n",
    "    },\n",
    "    'LR': {\n",
    "        'text': './rag_answers_llama.json',\n",
    "        'json': './rag_json_answers_llama.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "for b in answers:\n",
    "    for t in answers[b]:\n",
    "        with open(answers[b][t], 'r') as file:\n",
    "            answers[b][t] = {a['id']: a['content'] for a in json.loads(file.read())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_scores = {\n",
    "    'count': ['relative_error'],\n",
    "    'area': ['relative_error'],\n",
    "    'length': ['relative_error'],\n",
    "    'distance': ['relative_error'],\n",
    "    'name': ['P','R', 'F1'],\n",
    "    'loc': ['P','R', 'F1', 'distance_error'],\n",
    "    'angle': ['P','R', 'F1','angle_error']\n",
    "}\n",
    "for c in relevant_scores:\n",
    "    relevant_scores[c].append('attempted')\n",
    "\n",
    "scores = {}\n",
    "for b in text:\n",
    "    scores[b] = {'text': {}, 'json': {}}\n",
    "    for i, row in text[b].iterrows():\n",
    "        d = row.to_dict()\n",
    "        scores[b]['text'][row['id']] = {k: d[k] for k in d if k not in ['type', 'id']}\n",
    "for b in parsed:\n",
    "    for i, row in parsed[b].iterrows():\n",
    "        d = row.to_dict()\n",
    "        rs = []\n",
    "        for o in relevant_scores:\n",
    "            if d['type'].endswith(o):\n",
    "                rs = relevant_scores[o]\n",
    "                break\n",
    "        scores[b]['json'][row['id']] = {k: d[k] for k in d if (k not in ['type', 'id'] and k in rs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('./selected_questions/*.jsonl')\n",
    "questions = []\n",
    "for path in files:\n",
    "    question_type = path[path.rfind('/')+1:-6]\n",
    "    with open(path, 'r') as file:\n",
    "        for i in range(100):\n",
    "            line = file.readline()\n",
    "            question = json.loads(line)\n",
    "            question['type'] = question_type\n",
    "            questions.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def flatten_if_nested(array):\n",
    "    # Check if the input is a list and contains nested lists\n",
    "    if isinstance(array, list) and any(isinstance(item, list) for item in array):\n",
    "        flattened = []\n",
    "        for item in array:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(flatten_if_nested(item))\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "    else:\n",
    "        return array  # Return the input as-is if it's not a list or doesn't contain nested lists\n",
    "\n",
    "def extract_json_blocks(text, i):\n",
    "    # Regular expression pattern to match JSON blocks\n",
    "    pattern = r'```[\\s]*json(.*?)```'\n",
    "    pattern1 = r'\\b\\d+(?:_\\d+)*\\b'\n",
    "    pattern2 = r'\\b\\d+(?:,\\d+)*\\b'\n",
    "    pattern3 = r'//.*?\\n'\n",
    "    pattern4 = r',\\s*}'\n",
    "    pattern5 = r'}\\s*{'\n",
    "    # Find all JSON blocks\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Parse each match to ensure valid JSON\n",
    "    json_blocks = []\n",
    "    for match in matches:\n",
    "        try:\n",
    "            # Remove any leading/trailing whitespace and parse as JSON\n",
    "            s = match.strip()\n",
    "            s = re.sub(pattern1, lambda x: x.group().replace('_', ''), s)\n",
    "            s = re.sub(pattern2, lambda x: x.group().replace(',', ''), s)\n",
    "            s = re.sub(pattern3, '', s)\n",
    "            s = re.sub(pattern4, '}', s)\n",
    "            # these are just for corrected errors in the json strings\n",
    "            s = s.replace('''\\\\\\'''', '''\\'''').replace('''\\\\&''', '''&''').replace(\"\"\"\\\\'\"\"\", '''\\'''').replace('}\\njson', '}').replace('\" W', ' W').replace(\"\"\"\"length\": 20 + 30 + 10,\"\"\", \"\"\"\"length\": 60,\"\"\")\n",
    "            if re.search(pattern5, s):\n",
    "                s = re.sub(pattern5, '},\\n{', s)\n",
    "                s = '[\\n%s\\n]' % s\n",
    "            convert_area = False\n",
    "            if ' acres' in s:\n",
    "                convert_area = True\n",
    "                s = s.replace(' acres,', ',')\n",
    "            json_data = json.loads(s)\n",
    "            if convert_area and 'area' in json_data:\n",
    "                json_data['area'] = json_data['area'] * 4046.8564224\n",
    "            json_blocks.append(json_data)\n",
    "        except json.JSONDecodeError as w:\n",
    "            print(w)\n",
    "            # If parsing fails, print an error message (can log or handle as needed)\n",
    "            print(i)\n",
    "            print(s)\n",
    "            print(\"Warning: Found an invalid JSON block.\") \n",
    "    return flatten_if_nested(json_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entities(obj, q_type):\n",
    "    entitites = {}\n",
    "    for k in obj:\n",
    "        if k == '[1]':\n",
    "            o = {}\n",
    "            for k2 in obj[k]:\n",
    "                if k2 in ['main_category', 'sub_category', 'poi_filter_desc', 'poi_filter_sql', 'sub_category_label', 'table']:\n",
    "                    o[k2] = obj[k][k2]\n",
    "            entitites['[1]'] = o\n",
    "        else:\n",
    "            entitites[k] = obj[k]\n",
    "    return entitites\n",
    "\n",
    "def clean_question(q, q_type):\n",
    "    q = q.replace('  ', ' ')\n",
    "    q = q.replace(\"The\", \"the\")\n",
    "    if type_labels[q_type] in ['T4', 'T16', 'T20']:\n",
    "        q = q.replace(\"fast food\", \"fast food restaurant\")\n",
    "    if type_labels[q_type] == 'T16':\n",
    "        q = q.replace(\"where can I find\", \"where can I find  a\")\n",
    "    q = q.replace('Pediatric emergency', 'pediatric emergency')\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "counts = defaultdict(int)\n",
    "for q in questions:\n",
    "    counts[q['type']] += 1\n",
    "    baseline_answers = {}\n",
    "    for b in answers:\n",
    "        baseline_answers[b] = {}\n",
    "        baseline_answers[b]['text'] = {\n",
    "            'answer': answers[b]['text'][q['id']],\n",
    "            'scores': scores[b]['text'][q['id']]\n",
    "        }\n",
    "        baseline_answers[b]['parsed'] = {\n",
    "            'answer': extract_json_blocks(answers[b]['json'][int(q['id'])], q['id']),\n",
    "            'scores': scores[b]['json'][q['id']]\n",
    "        }\n",
    "    _q = {}\n",
    "    for k in q:\n",
    "        if k == 'question_entities':\n",
    "            _q[k] = clean_entities(q[k], q['type'])\n",
    "        elif k == 'question':\n",
    "            _q[k] = clean_question(q[k], q['type'])\n",
    "        else:\n",
    "            _q[k] = q[k]\n",
    "    path = './benchmark/%s/%3d/' % (type_labels[q['type']], counts[q['type']])\n",
    "    create_directory(path)\n",
    "    with open(path + 'question.json', 'w') as f:\n",
    "        f.write(json.dumps(_q, indent=2))\n",
    "    with open(path + 'baseline_answers.json', 'w') as f:\n",
    "        f.write(json.dumps(baseline_answers, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
